{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FeatureExtractionClassifier\n",
        "\n",
        "Use various features extracted from the WAV file to classify genres.\n",
        "Architecture implementation based on features derived at: https://www.kaggle.com/code/dramirdatascience/gtzan-music-classification-using-ml-acc-93-24"
      ],
      "metadata": {
        "collapsed": false,
        "id": "fc1cb74822640f0f"
      },
      "id": "fc1cb74822640f0f"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"ray[tune]\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from utils import *\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as Data\n",
        "from scipy.io import wavfile\n",
        "from ray import air\n",
        "import os\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import csv"
      ],
      "metadata": {
        "id": "nPN_ibZIW0sq"
      },
      "id": "nPN_ibZIW0sq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount drive\n",
        "Mount google drive if running on google colab"
      ],
      "metadata": {
        "id": "zIe4y2ZyXBs4"
      },
      "id": "zIe4y2ZyXBs4"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lv_sN8HDRsBw",
        "outputId": "9aa642d2-3290-495e-d2c7-ecb5deeb65a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lv_sN8HDRsBw",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constant parameters used in training\n",
        "\n",
        "Run `setup.sh` to mount Google Drive containing GTZAN"
      ],
      "metadata": {
        "id": "w-0VL_EBYDAe"
      },
      "id": "w-0VL_EBYDAe"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "GENRES = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3,\n",
        "          'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8,\n",
        "          'rock': 9}\n",
        "\n",
        "GTZAN_CSV = \"/content/drive/MyDrive/GTZAN/Data/features_3_sec.csv\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device\", DEVICE)"
      ],
      "metadata": {
        "id": "f541bc46d3b3c9b3"
      },
      "id": "f541bc46d3b3c9b3",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Create a `Dataset` for the audio files\n",
        "\n",
        "Obtain the features we are intested in from the WAV file. There are a total of 57 distinct features that are provided in the CSVs attached along with GTZAN. Using my understanding from [This analysis by the dataset author](https://www.kaggle.com/code/dramirdatascience/gtzan-music-classification-using-ml-acc-93-24), I have described the use of each of those features and how they were obtained in brief. For more in-depth understanding of how the features were obtained, the author has attached source code as well."
      ],
      "metadata": {
        "id": "eGkHns_KYILt"
      },
      "id": "eGkHns_KYILt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split each song into 10 parts as the features we extract are relevant only within a short time-frame, such as features regarding pitch/frequency.\n",
        "\n",
        "### Chroma STFT\n",
        "\n",
        "The same principle of using STFT to extract frequency magnitudes accross time-intervals as used in `mel_spec_classifier`. However, the frequency classes used now match the 12 pitch classes in western music instead. In effect, we end up looking at far less data and specifically the 12 pitches that are most useful when trying to understand genre.\n",
        "\n",
        "We obtain the mean and variance w.r.t magnitude to determine the base pitch of the song.\n",
        "\n",
        "### MFCC\n",
        "\n",
        "Again, same principle of using STFT to extract frequency magnitudes accross time-intervals as used in `mel_spec_classifier`. We don't  just analyze the spectogram here, but instead obtain the mean and variance of frequencies to again determine features regarding the frequency of the song.\n",
        "\n",
        "### RMS\n",
        "Average power of the song. Not unreasonable to theorize that Metal/Rock would be 'louder' on average.\n",
        "\n",
        "### Spectral Centroid Mean\n",
        "'Centre of mass' of frequencies in a time interval. Understanding of 'brightness' of a song, as higher C.O.M -> higher frequency -> perceived brightness.\n",
        "\n",
        "### Spectral Bandwith Mean\n",
        "Provides information on mean spread of frequencies.\n",
        "\n",
        "### Spectral Rolloff Mean\n",
        "Another metric of audio signal bandwidth. Gives frequency bin under which 50% of total energy exists.\n",
        "\n",
        "### Tempo Mean\n",
        "\n",
        "Tempo Mean calculates avg. perceived tempo by beat tracking, which essentailly measures power overtime and deterining BPM from the periodicity.\n",
        "\n",
        "### Zero Crossing Rate Mean\n",
        "\n",
        "Average rate that audio signal crosses zero axis over time. Tells us about rapid changes in audio signal\n",
        "\n",
        "### MFCC\n",
        "\n",
        "The Mel Spec\n",
        "\n"
      ],
      "metadata": {
        "id": "iX9E-mUaZgaN"
      },
      "id": "iX9E-mUaZgaN"
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractionDataset(Data.Dataset):\n",
        "  def __init__(self):\n",
        "    self.data = []\n",
        "    self.labels = []\n",
        "\n",
        "    # Go through all songs and tag X (tensor of features), Y as genre.\n",
        "    with open(GTZAN_CSV, newline='') as csvfile:\n",
        "      reader = csv.reader(csvfile)\n",
        "      for row in reader:\n",
        "        self.labels.append(GENRES[row[-1]])\n",
        "        # Contains all the features (comma seperated) as mentioned above\n",
        "        self.data.append(torch.tensor(row[2:-1], dtype=torch.float32))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "nJzp3hFEHoLO"
      },
      "id": "nJzp3hFEHoLO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the features, two fully connected layers is the preliminary model that we use. We do not fase a dimensionality issue as we did in the previous cases (`mel_spec` and `wav` classifiers)"
      ],
      "metadata": {
        "id": "G8fEzfSZYrBg"
      },
      "id": "G8fEzfSZYrBg"
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractionClassifier(nn.Module):\n",
        "  def __init__(self, l1=256, l2 = 20):\n",
        "    super().__init__()\n",
        "\n",
        "    # 57 input nodes as there are 57 features extracted.\n",
        "    self.fcs = nn.Sequential(\n",
        "        nn.Linear(57, l1),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(l1, l2),\n",
        "        nn.ReLU())\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fcs(x)"
      ],
      "metadata": {
        "id": "eRXt5CZtZIfm"
      },
      "id": "eRXt5CZtZIfm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create routines for training and validation. Perform Hyperparameter Tuning to devise a closer to optimized model."
      ],
      "metadata": {
        "id": "CZhdVHTNaHjB"
      },
      "id": "CZhdVHTNaHjB"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_feature_extraction_model(config):\n",
        "\n",
        "  model = FeatureExtractionClassifier(l1=config[\"l1\"], l2=config[\"l2\"])\n",
        "  model.to(DEVICE)\n",
        "  feature_extraction_dataset = FeatureExtractionDataset()\n",
        "\n",
        "  # train model with training dataset, but ray tuner uses validation dataset to tune hyperparameters\n",
        "    train_model(model, DEVICE, config, feature_extraction_dataset)"
      ],
      "metadata": {
        "id": "QpwR6HaUaPZy"
      },
      "id": "QpwR6HaUaPZy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "\n",
        " Create routine for testing model. The split being used is 80% for training, 10% for validation, and 10% for testing."
      ],
      "metadata": {
        "id": "-griptcGaqaD"
      },
      "id": "-griptcGaqaD"
    },
    {
      "cell_type": "code",
      "source": [
        "def test_feature_extraction_model(best_result):\n",
        "  best_model = FeatureExtractionClassifier(l1=best_result[\"l1\"], l2=best_result[\"l2\"])\n",
        "  best_model.to(DEVICE)\n",
        "\n",
        "  feature_extraction_dataset = FeatureExtractionDataset()\n",
        "  test_model(best_model, best_result, feature_extraction_dataset, DEVICE)"
      ],
      "metadata": {
        "id": "KpQY65uzax0i"
      },
      "id": "KpQY65uzax0i",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}