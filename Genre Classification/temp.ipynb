{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MelSpecClassifier\n",
    "\n",
    "Use the spectogram of the wav file and use a CNN with 2DConv to classify the genre.\n",
    "The spectograms have already been provided in the GTZAN dataset. It is cropped before being passed into the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7709b01bb59ec698"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as Data\n",
    "import os\n",
    "from PIL import ImageOps\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b5c1067407bf539"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constant parameters used in training\n",
    "\n",
    "Run `setup.sh` to mount Google Drive containing GTZAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aee620b52cdf21d7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "GTZAN_MEL = \"/content/drive/MyDrive/GTZAN/Data/images_original/\"\n",
    "\n",
    "PREPROCESS_CROP = (54, 35, 42, 35)\n",
    "\n",
    "IMAGE_INPUT_DIMENSIONS = [432, 288]\n",
    "GENRES = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3,\n",
    "          'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8,\n",
    "          'rock': 9}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53d3c0a7170272bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a `Dataset` for the mel-spectograms"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bff9cf4be9945421"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ImageDataset(Data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Go through all songs and tag X (tensor of image), Y as genre.\n",
    "        for genre in os.listdir(GTZAN_MEL):\n",
    "            for song in os.listdir(os.path.join(GTZAN_MEL, genre)):\n",
    "                abs_path = os.path.join(GTZAN_MEL, genre, song)\n",
    "                image = Image.open(abs_path)\n",
    "\n",
    "                # The images have been obtained in the dataset by using the mel spectogram (librosa)\n",
    "                # Cropping the image to only contain the spectogram to pass into CNN\n",
    "                image_cropped = ImageOps.crop(image, PREPROCESS_CROP)\n",
    "\n",
    "                transform = transforms.Compose([transforms.ToTensor()])\n",
    "                # Convert PIL Image to tensor\n",
    "                self.images.append(transform(image_cropped))\n",
    "                # Convert genre tag to associated digit\n",
    "                self.labels.append(GENRES[genre])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed4bbc274edae1fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `MelSpecTrainer` model used is a CNN with 2 convolutional layers and 2 linear layers.\n",
    "There is a lack of datapoints compared to the number of dimensions.\n",
    "To avoid over-training, output features of the linear layer is less, and the number of layers is 2."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "911db241ef0366b6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MelSpecTrainer(nn.Module):\n",
    "    def __init__(self, l1=256, l2=20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.current_dimensions = IMAGE_INPUT_DIMENSIONS\n",
    "\n",
    "        self.conv_layer_1 = nn.Sequential(nn.Conv2d(4, 32, 3),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.MaxPool2d(kernel_size=2, stride=3)\n",
    "                                          )\n",
    "\n",
    "        self.conv_layer_2 = nn.Sequential(nn.Conv2d(32, 16, 3),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.MaxPool2d(kernel_size=2, stride=3)\n",
    "                                          )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.linear_layer_1 = nn.Sequential(nn.Linear(12320, l1),\n",
    "                                            nn.ReLU())\n",
    "\n",
    "        self.linear_layer_2 = nn.Sequential(nn.Linear(l1, l2),\n",
    "                                            nn.ReLU())\n",
    "\n",
    "        self.classifier = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First 2D convolution layer\n",
    "        x = self.conv_layer_1(x)\n",
    "        # Second 2D convolution layer\n",
    "        x = self.conv_layer_2(x)\n",
    "\n",
    "        # Linear layer and classifier\n",
    "        x = self.flatten_layer(x)\n",
    "        x = self.linear_layer_1(x)\n",
    "        x = self.linear_layer_2(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce29243b72c7fedf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the mel spectrogram to train a model for classification of genre of the wav file\n",
    "Split into test/train/validation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "820d5cc7dcbf4429"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def dataset_split(image_dataset):\n",
    "    indices = list(range(len(image_dataset)))\n",
    "    random.seed(42)\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    num_train = int(len(image_dataset) * 0.8)\n",
    "    num_validation = int(len(image_dataset) * 0.1)\n",
    "    train_indices = indices[:num_train]\n",
    "    test_and_validation = indices[num_train:]\n",
    "    validation_indices = test_and_validation[:num_validation]\n",
    "    test_indices = test_and_validation[num_validation:]\n",
    "    \n",
    "    return test_indices, train_indices, validation_indices"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91d13e469b56735"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create routines for training and validation. Perform Hyperparameter Tuning to devise a closer to optimized model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b72560113677293"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_mel_spec_model(config):\n",
    "    \n",
    "    model = MelSpecTrainer(l1=config[\"l1\"], l2=config[\"l2\"])\n",
    "    model.to(DEVICE)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), config[\"l1\"])\n",
    "    image_dataset = ImageDataset()\n",
    "    \n",
    "    _, train_indices, validation_indices = dataset_split(image_dataset)\n",
    "    \n",
    "    # Create test and train datasets\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "    \n",
    "    train_dataset = Data.DataLoader(image_dataset, batch_size=config[\"batch_size\"], sampler=train_sampler)\n",
    "    \n",
    "    validation_dataset = Data.DataLoader(image_dataset, batch_size=config[\"batch_size\"], sampler=validation_sampler)\n",
    "    \n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        for batch_id, curr_batch in enumerate(train_dataset):\n",
    "            # Predict and get loss\n",
    "            images, labels = curr_batch[0].to(DEVICE), curr_batch[1].to(DEVICE)\n",
    "            pred = model(images)\n",
    "            loss = loss_fn(pred, labels)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"epoch: {epoch}, batch_id: {batch_id}, loss: {loss}\")\n",
    "    \n",
    "        # Validation loss\n",
    "        # Calculate avg.loss and accuracy for all datapoints in validation set.\n",
    "        # Based on https://docs.ray.io/en/latest/tune/examples/tune-pytorch-cifar.html\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in validation_dataset:\n",
    "            with torch.no_grad():\n",
    "                images, labels = data\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "    \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss\n",
    "                val_steps += 1\n",
    "                \n",
    "        # Construct checkpoint for Ray Tuner\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            path = os.path.join(temp_checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(\n",
    "                (model.state_dict(), optimizer.state_dict()), path\n",
    "            )\n",
    "            checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "            train.report(\n",
    "                {\"loss\": (val_loss / val_steps), \"accuracy\": correct / total},\n",
    "                checkpoint=checkpoint,\n",
    "            )\n",
    "    \n",
    "        print(f\"Validation Loss: {val_loss / val_steps}, Accuracy: {correct / total}\")\n",
    "            "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec7098de5d7f8d80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create routine for testing model. The split being used is 80% for training, 10% for validation, and 10% for testing. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78422c7087cb7d13"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_mel_spec_model(best_result):\n",
    "    best_model = MelSpecTrainer(l1=best_result.config[\"l1\"], l2=best_result.config[\"l2\"])\n",
    "    best_model.to(DEVICE)\n",
    "    \n",
    "    # Get model state from best result\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "    model_state, _ = best_model.load(checkpoint_path)\n",
    "    image_dataset = ImageDataset()\n",
    "    \n",
    "    # Create test data loader\n",
    "    test_indices, _, _ = dataset_split(image_dataset)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "    test_dataset = Data.DataLoader(image_dataset, batch_size=5, sampler=test_sampler)\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data in test_dataset:\n",
    "        with torch.no_grad():\n",
    "            images, labels = data\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = best_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f\"Best Model Accuracy {(correct * 100) / total}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd0d2f7156bd62c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main function\n",
    "\n",
    "Here, we specify the range for the hyperparameters we want Ray Tune to tune on. Run the training of the model using various hyperparameters.\n",
    "\n",
    "Test the model using the best trained model as obtained using Ray Tune"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a02c125476a900be"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_mel_spec_classifier():\n",
    "    config = {\n",
    "        \"l1\": ..., \n",
    "        \"l2\": ...,\n",
    "        \"lr\": ...,\n",
    "        \"batch_size\": ...\n",
    "    }\n",
    "    \n",
    "    # Only stop trials at least after 20 training iterations\n",
    "    asha_scheduler = ASHAScheduler(time_attr='training_iteration',\n",
    "                                   grace_period=20)\n",
    "    \n",
    "    tuner = tune.Tuner(tune.with_resources(tune.with_parameters(train_mel_spec_model)),\n",
    "                       tune_resources=tune.TuneConfig(\n",
    "                           metric='loss',\n",
    "                           mode=\"min\",\n",
    "                           scheduler=asha_scheduler,\n",
    "                           num_samples=5,\n",
    "                       ),\n",
    "                       param_space=config,)\n",
    "    \n",
    "    results = tuner.fit()\n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "    \n",
    "    test_mel_spec_model(best_result)\n",
    "    \n",
    "run_mel_spec_classifier()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6968c34e8c7980"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
