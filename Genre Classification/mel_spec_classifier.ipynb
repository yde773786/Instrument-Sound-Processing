{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ebcadd2-782c-4e51-a177-6ba2c06ae30b",
      "metadata": {
        "id": "9ebcadd2-782c-4e51-a177-6ba2c06ae30b"
      },
      "source": [
        "# MelSpecClassifier\n",
        "\n",
        "Use the spectogram of the wav file and use a CNN with 2DConv to classify the genre.\n",
        "The spectograms have already been provided in the GTZAN dataset. It is cropped before being passed into the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d766c0-a752-47d6-9e1c-cce431d0d769",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9d766c0-a752-47d6-9e1c-cce431d0d769",
        "outputId": "e1aac748-2201-4ccc-bd82-a8acc43b84c7"
      },
      "outputs": [],
      "source": [
        "!pip install \"ray[tune]\"\n",
        "import torch\n",
        "from utils import *\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import tempfile\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as Data\n",
        "import os\n",
        "from PIL import ImageOps\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from ray import train, tune, air\n",
        "from ray.train import Checkpoint\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5NEFVi9Tt22",
      "metadata": {
        "id": "f5NEFVi9Tt22"
      },
      "source": [
        "## Mount drive\n",
        "Mount google drive if running on google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IvyuycZ3PbVm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvyuycZ3PbVm",
        "outputId": "c08d0d5e-235c-4380-af78-200e9089eb27"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3c357c-4679-41b0-8df3-8e4552f6bf90",
      "metadata": {
        "id": "df3c357c-4679-41b0-8df3-8e4552f6bf90"
      },
      "source": [
        "## Constant parameters used in training\n",
        "\n",
        "Run `setup.sh` to mount Google Drive containing GTZAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b1722aad-12e4-4c48-8dd2-e28dd5aa520c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1722aad-12e4-4c48-8dd2-e28dd5aa520c",
        "outputId": "2525c809-2520-4a21-e531-57807c2b7380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cpu\n"
          ]
        }
      ],
      "source": [
        "GTZAN_MEL = \"/content/drive/MyDrive/GTZAN/Data/images_original/\"\n",
        "\n",
        "PREPROCESS_CROP = (54, 35, 42, 35)\n",
        "\n",
        "IMAGE_INPUT_DIMENSIONS = [432, 288]\n",
        "GENRES = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3,\n",
        "          'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8,\n",
        "          'rock': 9}\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e25ed03-399e-4dfd-bcf5-141290780bd3",
      "metadata": {
        "id": "6e25ed03-399e-4dfd-bcf5-141290780bd3"
      },
      "source": [
        "## Training\n",
        "\n",
        "Create a `Dataset` for the mel-spectograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1fae4bc6-749b-4eb3-91cf-8a693f7aea0c",
      "metadata": {
        "id": "1fae4bc6-749b-4eb3-91cf-8a693f7aea0c"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Data.Dataset):\n",
        "    def __init__(self):\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Go through all songs and tag X (tensor of image), Y as genre.\n",
        "        for genre in os.listdir(GTZAN_MEL):\n",
        "            for song in os.listdir(os.path.join(GTZAN_MEL, genre)):\n",
        "                abs_path = os.path.join(GTZAN_MEL, genre, song)\n",
        "                image = Image.open(abs_path)\n",
        "\n",
        "                # The images have been obtained in the dataset by using the mel spectogram (librosa)\n",
        "                # Cropping the image to only contain the spectogram to pass into CNN\n",
        "                image_cropped = ImageOps.crop(image, PREPROCESS_CROP)\n",
        "\n",
        "                transform = transforms.Compose([transforms.ToTensor()])\n",
        "                # Convert PIL Image to tensor\n",
        "                self.images.append(transform(image_cropped))\n",
        "                # Convert genre tag to associated digit\n",
        "                self.labels.append(GENRES[genre])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae7756e-a2cd-4a88-9509-172c8b7bc0ef",
      "metadata": {
        "id": "9ae7756e-a2cd-4a88-9509-172c8b7bc0ef"
      },
      "source": [
        "The `MelSpecTrainer` model used is a CNN with 2 convolutional layers and 2 linear layers.\n",
        "- There is a lack of datapoints compared to the number of dimensions.\n",
        "- The true dimensionality of each genre appears to be in the order of 10\n",
        "- There are no further hidden layers as a balance between complexity of network and risk of overtraining.\n",
        "- The kernel size and number of convolutional layers follow empirically determined industry norms for general image detection and classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "65dd676c-5fd9-4799-821a-9074f41ffafb",
      "metadata": {
        "id": "65dd676c-5fd9-4799-821a-9074f41ffafb"
      },
      "outputs": [],
      "source": [
        "class MelSpecTrainer(nn.Module):\n",
        "    def __init__(self, l1=256, l2=20):\n",
        "        super().__init__()\n",
        "\n",
        "        self.current_dimensions = IMAGE_INPUT_DIMENSIONS\n",
        "\n",
        "        self.conv_layer_1 = nn.Sequential(nn.Conv2d(4, 32, 3),\n",
        "                                          nn.ReLU(),\n",
        "                                          nn.MaxPool2d(kernel_size=2, stride=3)\n",
        "                                          )\n",
        "\n",
        "        self.conv_layer_2 = nn.Sequential(nn.Conv2d(32, 16, 3),\n",
        "                                          nn.ReLU(),\n",
        "                                          nn.MaxPool2d(kernel_size=2, stride=3)\n",
        "                                          )\n",
        "\n",
        "        self.flatten_layer = nn.Flatten()\n",
        "\n",
        "        self.linear_layer_1 = nn.Sequential(nn.Linear(13248, l1),\n",
        "                                            nn.ReLU())\n",
        "\n",
        "        self.linear_layer_2 = nn.Sequential(nn.Linear(l1, l2),\n",
        "                                            nn.ReLU())\n",
        "\n",
        "        self.classifier = nn.Linear(l2, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First 2D convolution layer\n",
        "        x = self.conv_layer_1(x)\n",
        "        # Second 2D convolution layer\n",
        "        x = self.conv_layer_2(x)\n",
        "\n",
        "        # Linear layer and classifier\n",
        "        x = self.flatten_layer(x)\n",
        "        x = self.linear_layer_1(x)\n",
        "        x = self.linear_layer_2(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d1c9f89649d86123",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1c9f89649d86123",
        "is_executing": true,
        "outputId": "4b2fdd7f-ccdb-45f7-a92d-192fcb4f0811"
      },
      "outputs": [],
      "source": [
        "for i in range(torch.cuda.device_count()):\n",
        "   print(torch.cuda.get_device_properties(i).name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0676965e-5c31-40ea-ad7a-e31c8f8d8f06",
      "metadata": {
        "id": "0676965e-5c31-40ea-ad7a-e31c8f8d8f06"
      },
      "source": [
        "Create routines for training and validation. Perform Hyperparameter Tuning to devise a closer to optimized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d5575b4c-1ecd-4f6e-a5a3-4e79951f43cf",
      "metadata": {
        "id": "d5575b4c-1ecd-4f6e-a5a3-4e79951f43cf"
      },
      "outputs": [],
      "source": [
        "def train_mel_spec_model(config):\n",
        "\n",
        "    model = MelSpecTrainer(l1=config[\"l1\"], l2=config[\"l2\"])\n",
        "    model.to(DEVICE)\n",
        "    image_dataset = ImageDataset()\n",
        "\n",
        "    # train model with training dataset, but ray tuner uses validation dataset to tune hyperparameters\n",
        "    train_model(model, DEVICE, config, image_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c95d85c3-78ec-4ed3-b729-e99faa77eebf",
      "metadata": {
        "id": "c95d85c3-78ec-4ed3-b729-e99faa77eebf"
      },
      "source": [
        "## Testing\n",
        "\n",
        " Create routine for testing model. The split being used is 80% for training, 10% for validation, and 10% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed01392-a18d-4209-a424-b438170562a5",
      "metadata": {
        "id": "6ed01392-a18d-4209-a424-b438170562a5"
      },
      "outputs": [],
      "source": [
        "def test_mel_spec_model(best_result):\n",
        "    best_model = MelSpecTrainer(l1=best_result.config[\"l1\"], l2=best_result.config[\"l2\"])\n",
        "    best_model.to(DEVICE)\n",
        "\n",
        "    image_dataset = ImageDataset()\n",
        "\n",
        "    test_model(best_model, best_result, image_dataset, DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0ef4801-0023-4240-9c99-0f5dd639a45e",
      "metadata": {
        "id": "f0ef4801-0023-4240-9c99-0f5dd639a45e"
      },
      "source": [
        "# Main function\n",
        "\n",
        "Here, we specify the range for the hyperparameters we want Ray Tune to tune on. Run the training of the model using various hyperparameters.\n",
        "\n",
        "Test the model using the best trained model as obtained using Ray Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a60d334-49ec-465e-9442-1c8a9687ec55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a60d334-49ec-465e-9442-1c8a9687ec55",
        "outputId": "31055f84-59f2-444c-e993-0f49cf76c28d"
      },
      "outputs": [],
      "source": [
        "def run_mel_spec_classifier():\n",
        "    config = {\n",
        "        \"l1\": tune.choice([k for k in range(100, 1000, 50)]),\n",
        "        \"l2\": tune.choice([j for j in range(5, 30, 5)]),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([l for l in range(5, 50, 5)]),\n",
        "        \"num_epochs\": tune.choice([25, 35, 45])\n",
        "    }\n",
        "\n",
        "    # Only stop trials at least after 20 training iterations\n",
        "    asha_scheduler = ASHAScheduler(time_attr='training_iteration',\n",
        "                                   grace_period=20)\n",
        "\n",
        "    # Adjust resources depending on availability\n",
        "    tuner = tune.Tuner(tune.with_resources(tune.with_parameters(train_mel_spec_model),\n",
        "                       resources={\"cpu\": 2, \"gpu\": 1}),\n",
        "                       tune_config=tune.TuneConfig(\n",
        "                           metric='loss',\n",
        "                           mode=\"min\",\n",
        "                           scheduler=asha_scheduler,\n",
        "                           num_samples=5,\n",
        "                       ),\n",
        "                       run_config=air.RunConfig(verbose=1)\n",
        "                       param_space=config,)\n",
        "\n",
        "    results = tuner.fit()\n",
        "    best_result = results.get_best_result(\"loss\", \"min\")\n",
        "\n",
        "    test_mel_spec_model(best_result)\n",
        "\n",
        "run_mel_spec_classifier()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
