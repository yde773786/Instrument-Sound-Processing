{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-23T22:11:44.187458614Z",
     "start_time": "2024-03-23T22:11:43.706204631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the spectogram of the wav file and use a CNN with 2DConv to classify the genre.\n",
    "# The spectograms have already been provided in the GTZAN dataset. It is cropped before being passed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as Data\n",
    "import os\n",
    "from PIL import ImageOps\n",
    "from torch.utils.data import SubsetRandomSampler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T15:32:44.543182Z",
     "start_time": "2024-03-24T15:32:42.340579Z"
    }
   },
   "id": "3dc25ba0ff3ae48e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constant parameters used in training\n",
    "\n",
    "Run `setup.sh` to mount Google Drive containing GTZAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89890ad0ce4a88ce"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "GTZAN_MEL = \"/content/drive/MyDrive/GTZAN/Data/images_original/\"\n",
    "\n",
    "PREPROCESS_CROP = (54, 35, 42, 35)\n",
    "\n",
    "IMAGE_INPUT_DIMENSIONS = [432, 288]\n",
    "GENRES = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3,\n",
    "          'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8,\n",
    "          'rock': 9}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T15:32:52.699331Z",
     "start_time": "2024-03-24T15:32:52.695099Z"
    }
   },
   "id": "f830e69a2914af72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a `Dataset` for the mel-spectograms"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9a2855393aaa486"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ImageDataset(Data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Go through all songs and tag X (tensor of image), Y as genre.\n",
    "        for genre in os.listdir(GTZAN_MEL):\n",
    "            for song in os.listdir(os.path.join(GTZAN_MEL, genre)):\n",
    "                abs_path = os.path.join(GTZAN_MEL, genre, song)\n",
    "                image = Image.open(abs_path)\n",
    "\n",
    "                # The images have been obtained in the dataset by using the mel spectogram (librosa)\n",
    "                # Cropping the image to only contain the spectogram to pass into CNN\n",
    "                image_cropped = ImageOps.crop(image, PREPROCESS_CROP)\n",
    "\n",
    "                transform = transforms.Compose([transforms.ToTensor()])\n",
    "                # Convert PIL Image to tensor\n",
    "                self.images.append(transform(image_cropped))\n",
    "                # Convert genre tag to associated digit\n",
    "                self.labels.append(GENRES[genre])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T15:32:55.925969Z",
     "start_time": "2024-03-24T15:32:55.921184Z"
    }
   },
   "id": "db6325d3be5d3233",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `MelSpecTrainer` model used is a CNN with 2 convolutional layers and 2 linear layers.\n",
    "There is a lack of datapoints compared to the number of dimensions.\n",
    "To avoid over-training, output features of the linear layer is less, and the number of layers is 2."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73c8992421d51e32"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MelSpecTrainer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.current_dimensions = IMAGE_INPUT_DIMENSIONS\n",
    "\n",
    "        self.conv_layer_1 = nn.Sequential(nn.Conv2d(4, 32, 3),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.MaxPool2d(kernel_size=2, stride=3)\n",
    "                                          )\n",
    "\n",
    "        self.conv_layer_2 = nn.Sequential(nn.Conv2d(32, 16, 3),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.MaxPool2d(kernel_size=2, stride=3)\n",
    "                                          )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.linear_layer_1 = nn.Sequential(nn.Linear(12320, 256),\n",
    "                                            nn.ReLU())\n",
    "\n",
    "        self.linear_layer_2 = nn.Sequential(nn.Linear(256, 20),\n",
    "                                            nn.ReLU())\n",
    "\n",
    "        self.classifier = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First 2D convolution layer\n",
    "        x = self.conv_layer_1(x)\n",
    "        # Second 2D convolution layer\n",
    "        x = self.conv_layer_2(x)\n",
    "\n",
    "        # Linear layer and classifier\n",
    "        x = self.flatten_layer(x)\n",
    "        x = self.linear_layer_1(x)\n",
    "        x = self.linear_layer_2(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15f398324a3f8c2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the mel spectrogram to train a model for classification of genre of the wav file\n",
    "Split into test/train/validation. Create corresonding `DataLoaders`\n",
    "Create routines for training and testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c6f0e899f89a655"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MelSpecApproachClassifier:\n",
    "            \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = 0, 0, 0, 0\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        print(\"Using device\", self.device)\n",
    "        \n",
    "        image_dataset = ImageDataset()\n",
    "        self.model = MelSpecTrainer()\n",
    "        self.model.to(self.device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        # shuffle dataset before splitting into test/train datasets\n",
    "        indices = list(range(len(image_dataset)))\n",
    "        random.seed(42)\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        # 80% dataset for training, 10% for validation, and 10% for testing.\n",
    "        num_train = int(len(image_dataset) * 0.8)\n",
    "        num_validation = int(len(image_dataset) * 0.1)\n",
    "\n",
    "        train_indices = indices[:num_train]\n",
    "        test_and_validation = indices[num_train:]\n",
    "        validation_indices = test_and_validation[:num_validation]\n",
    "        test_indices = test_and_validation[num_validation:]\n",
    "\n",
    "        # Create test and train datasets\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "        self.train_dataset = Data.DataLoader(image_dataset, batch_size=5, sampler=train_sampler)\n",
    "        self.validation_dataset = Data.DataLoader(image_dataset, sampler=validation_sampler)\n",
    "        self.test_dataset = Data.DataLoader(image_dataset, sampler=test_sampler)\n",
    "        \n",
    "    def train_model(self):\n",
    "        for epoch in range(50):\n",
    "            for batch_id, curr_batch in enumerate(self.train_dataset):\n",
    "                # Predict and get loss\n",
    "                images, labels = curr_batch[0].to(self.device), curr_batch[1].to(self.device)\n",
    "                pred = self.model(images)\n",
    "                loss = self.loss_fn(pred, labels)\n",
    "    \n",
    "                # backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "                print(f\"epoch: {epoch}, batch_id: {batch_id}, loss: {loss}\")\n",
    "\n",
    "    def test_model(self, type_dataset):\n",
    "        # evaluation mode\n",
    "        self.model.eval()\n",
    "        correct_cnt = 0\n",
    "\n",
    "        if type_dataset.lower() == \"test\":\n",
    "            dataset = self.test_dataset\n",
    "        elif type_dataset.lower() == \"validation\":\n",
    "            dataset = self.validation_dataset\n",
    "        else:\n",
    "            dataset = self.train_dataset\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in dataset:\n",
    "\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                pred = self.model(images)\n",
    "\n",
    "                # Correctly classified genre of song snippet\n",
    "                _, predicted = torch.max(pred, 1)\n",
    "                correct_cnt += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"{type_dataset} Accuracy: {correct_cnt / (len(dataset) * dataset.batch_size)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T16:13:19.260165Z",
     "start_time": "2024-03-24T16:13:19.251827Z"
    }
   },
   "id": "d4d70e5de7d8bbed",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c50b67d60c02455"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mel_spec_approach_classifier = MelSpecApproachClassifier()\n",
    "mel_spec_approach_classifier.train_model()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97e4869c9e8e2172",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test the model on all Datasets (Test/Train/Validation)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cb298fa0d5d41b7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mel_spec_approach_classifier.test_model(type_dataset=\"Train\")\n",
    "mel_spec_approach_classifier.test_model(type_dataset=\"Validation\")\n",
    "mel_spec_approach_classifier.test_model(type_dataset=\"Test\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb2b185a7e92c174"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
