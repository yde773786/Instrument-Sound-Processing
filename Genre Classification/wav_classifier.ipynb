{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# WavClassifier\n",
    "\n",
    "Use the WAV file directly; extract features with a CNN using 1DConv.\n",
    "State-of-the-art audio classifiers use Mel-Spectograms as described in `./mel_spec_classifier.ipynb`, but do not  preserve phase information"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44ea56c940cf73cb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import tempfile\n",
    "from scipy.io import wavfile\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as Data\n",
    "import os\n",
    "from PIL import ImageOps\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "983a6f76208e19a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constant parameters used in training\n",
    "\n",
    "Run `setup.sh` to mount Google Drive containing GTZAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dec4c015eef3f17d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "GTZAN_WAV = \"/content/drive/MyDrive/GTZAN/Data/genres_original/\"\n",
    "\n",
    "GENRES = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3,\n",
    "          'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8,\n",
    "          'rock': 9}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "936601a226e691e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a `Dataset` for the audio files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b8f41e4d044bd12"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class WAVDataset(Data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.wav = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Go through all songs and tag X (tensor of image), Y as genre.\n",
    "        for genre in os.listdir(GTZAN_WAV):\n",
    "            for song in os.listdir(os.path.join(GTZAN_WAV, genre)):\n",
    "                abs_path = os.path.join(GTZAN_WAV, genre, song)\n",
    "                _, data = wavfile.read(song) \n",
    "\n",
    "                # Convert PIL Image to tensor\n",
    "                self.wav.append(torch.from_numpy(abs_path))\n",
    "                # Convert genre tag to associated digit\n",
    "                self.labels.append(GENRES[genre])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.wav[idx], self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7815db0d76e8aea2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `WavTrainer` model used is a CNN with 2 convolutional layers and 2 linear layers.\n",
    "The justifications for the architecture are consistent with `mel_spec_classifier.ipynb`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fda583a987e91248"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
