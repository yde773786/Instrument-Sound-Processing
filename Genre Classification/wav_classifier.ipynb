{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WavClassifier\n",
        "\n",
        "Use the WAV file directly; extract features with a CNN using 1DConv.\n",
        "State-of-the-art audio classifiers use Mel-Spectograms as described in `./mel_spec_classifier.ipynb`, but do not  preserve phase information"
      ],
      "metadata": {
        "collapsed": false,
        "id": "44ea56c940cf73cb"
      },
      "id": "44ea56c940cf73cb"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "!pip install \"ray[tune]\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import tempfile\n",
        "from scipy.io import wavfile\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as Data\n",
        "import os\n",
        "from PIL import ImageOps\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from ray import train, tune\n",
        "from ray.train import Checkpoint\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ],
      "metadata": {
        "id": "983a6f76208e19a9"
      },
      "id": "983a6f76208e19a9",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mount drive\n",
        "Mount google drive if running on google colab"
      ],
      "metadata": {
        "id": "YSzR7vjgRYsN"
      },
      "id": "YSzR7vjgRYsN"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qmTopAHiRa0a"
      },
      "id": "qmTopAHiRa0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constant parameters used in training\n",
        "\n",
        "Run `setup.sh` to mount Google Drive containing GTZAN"
      ],
      "metadata": {
        "collapsed": false,
        "id": "dec4c015eef3f17d"
      },
      "id": "dec4c015eef3f17d"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "GTZAN_WAV = \"/content/drive/MyDrive/GTZAN/Data/genres_original/\"\n",
        "\n",
        "GENRES = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3,\n",
        "          'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8,\n",
        "          'rock': 9}\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device\", DEVICE)"
      ],
      "metadata": {
        "id": "936601a226e691e2"
      },
      "id": "936601a226e691e2",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Create a `Dataset` for the audio files"
      ],
      "metadata": {
        "collapsed": false,
        "id": "5b8f41e4d044bd12"
      },
      "id": "5b8f41e4d044bd12"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "class WAVDataset(Data.Dataset):\n",
        "    def __init__(self):\n",
        "        self.wav = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Go through all songs and tag X (tensor of image), Y as genre.\n",
        "        for genre in os.listdir(GTZAN_WAV):\n",
        "            for song in os.listdir(os.path.join(GTZAN_WAV, genre)):\n",
        "                abs_path = os.path.join(GTZAN_WAV, genre, song)\n",
        "                _, data = wavfile.read(abs_path)\n",
        "\n",
        "                # Convert PIL Image to tensor\n",
        "                self.wav.append(torch.from_numpy(data))\n",
        "                # Convert genre tag to associated digit\n",
        "                self.labels.append(GENRES[genre])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wav)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.wav[idx], self.labels[idx]\n",
        "\n",
        "image_dataset = WAVDataset()"
      ],
      "metadata": {
        "id": "7815db0d76e8aea2"
      },
      "id": "7815db0d76e8aea2",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `WavTrainer` model used is a CNN with 2 convolutional layers and 2 linear layers.\n",
        "\n",
        "Each `wav` file is 30 seconds long and sampled at 22050 Hz. So, we have datapoints of size: ~661500. As humans. We make an estimation from a human standpoint regarding how long 'musical features' are to differentiate genres. The smallest 'features' seem to be differentiable within a significant fraction of a second.\n",
        "\n",
        "So, the receptive field of the convolutional layer of the CNN should cover\n",
        "a significant fraction of a second.\n",
        "\n",
        "What is a significant fraction of a second? This is on hyperparameter tuning to decide. But the conclusion is that very small kernel sizes (such as 3 in `2DConv`) should not apply here since we wouldn't obtain much about the song features itself through essentially 0.0001 seconds of the song.\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "fda583a987e91248"
      },
      "id": "fda583a987e91248"
    },
    {
      "cell_type": "code",
      "source": [
        "class WavTrainer(nn.Module):\n",
        "  def __init__(self, l1=256, l2=20):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv_layer_1 = nn.Sequential(nn.Conv1d(1, 32, 20),\n",
        "                                      nn.ReLU(),\n",
        "                                      nn.MaxPool1d(kernel_size=10, stride=10)\n",
        "                                      )\n",
        "\n",
        "    self.conv_layer_2 = nn.Sequential(nn.Conv1d(32, 16, 20),\n",
        "                                      nn.ReLU(),\n",
        "                                      nn.MaxPool1d(kernel_size=10, stride=10)\n",
        "                                      )\n",
        "\n",
        "    self.flatten_layer = nn.Flatten()\n",
        "\n",
        "    self.linear_layer_1 = nn.Sequential(nn.Linear(13248, l1),\n",
        "                                        nn.ReLU())\n",
        "\n",
        "    self.linear_layer_2 = nn.Sequential(nn.Linear(l1, l2),\n",
        "                                        nn.ReLU())\n",
        "\n",
        "    self.classifier = nn.Linear(l2, 10)\n",
        "\n",
        "def forward(self, x):\n",
        "    # First 1D convolution layer\n",
        "    x = self.conv_layer_1(x)\n",
        "    # Second 1D convolution layer\n",
        "    x = self.conv_layer_2(x)\n",
        "\n",
        "    # Linear layer and classifier\n",
        "    x = self.flatten_layer(x)\n",
        "    x = self.linear_layer_1(x)\n",
        "    x = self.linear_layer_2(x)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "_eGEAlmMP4Rf"
      },
      "id": "_eGEAlmMP4Rf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into test/train/validation"
      ],
      "metadata": {
        "id": "0XUKm25bZwnD"
      },
      "id": "0XUKm25bZwnD"
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_split(wav_dataset):\n",
        "    indices = list(range(len(wav_dataset)))\n",
        "    random.seed(42)\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    num_train = int(len(wav_dataset) * 0.8)\n",
        "    num_validation = int(len(wav_dataset) * 0.1)\n",
        "    train_indices = indices[:num_train]\n",
        "    test_and_validation = indices[num_train:]\n",
        "    validation_indices = test_and_validation[:num_validation]\n",
        "    test_indices = test_and_validation[num_validation:]\n",
        "\n",
        "    return test_indices, train_indices, validation_indices"
      ],
      "metadata": {
        "id": "ABuRlHHQaZQV"
      },
      "id": "ABuRlHHQaZQV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}