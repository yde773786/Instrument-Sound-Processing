{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "44ea56c940cf73cb",
      "metadata": {
        "collapsed": false,
        "id": "44ea56c940cf73cb"
      },
      "source": [
        "# WavClassifier\n",
        "\n",
        "Use the WAV file directly; extract features with a CNN using 1DConv.\n",
        "State-of-the-art audio classifiers use Mel-Spectograms as described in `./mel_spec_classifier.ipynb`, but do not  preserve phase information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "983a6f76208e19a9",
      "metadata": {
        "id": "983a6f76208e19a9"
      },
      "outputs": [],
      "source": [
        "!pip install \"ray[tune]\"\n",
        "import torch\n",
        "from utils import *\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as Data\n",
        "import os\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YSzR7vjgRYsN",
      "metadata": {
        "id": "YSzR7vjgRYsN"
      },
      "source": [
        "## Mount drive\n",
        "Mount google drive if running on google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qmTopAHiRa0a",
      "metadata": {
        "id": "qmTopAHiRa0a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dec4c015eef3f17d",
      "metadata": {
        "collapsed": false,
        "id": "dec4c015eef3f17d"
      },
      "source": [
        "## Constant parameters used in training\n",
        "\n",
        "Run `setup.sh` to mount Google Drive containing GTZAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "936601a226e691e2",
      "metadata": {
        "id": "936601a226e691e2"
      },
      "outputs": [],
      "source": [
        "GTZAN_WAV = \"/content/drive/MyDrive/GTZAN/Data/genres_original/\"\n",
        "\n",
        "GENRES = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3,\n",
        "          'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8,\n",
        "          'rock': 9}\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8f41e4d044bd12",
      "metadata": {
        "collapsed": false,
        "id": "5b8f41e4d044bd12"
      },
      "source": [
        "## Training\n",
        "\n",
        "Create a `Dataset` for the audio files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7815db0d76e8aea2",
      "metadata": {
        "id": "7815db0d76e8aea2"
      },
      "outputs": [],
      "source": [
        "class WAVDataset(Data.Dataset):\n",
        "    def __init__(self):\n",
        "        self.wav = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Go through all songs and tag X (tensor of image), Y as genre.\n",
        "        for genre in os.listdir(GTZAN_WAV):\n",
        "            for song in os.listdir(os.path.join(GTZAN_WAV, genre)):\n",
        "                abs_path = os.path.join(GTZAN_WAV, genre, song)\n",
        "                _, data = wavfile.read(abs_path)\n",
        "\n",
        "                # Convert PIL Image to tensor\n",
        "                self.wav.append(torch.from_numpy(data))\n",
        "                # Convert genre tag to associated digit\n",
        "                self.labels.append(GENRES[genre])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wav)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.wav[idx], self.labels[idx]\n",
        "\n",
        "image_dataset = WAVDataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda583a987e91248",
      "metadata": {
        "collapsed": false,
        "id": "fda583a987e91248"
      },
      "source": [
        "The `WavTrainer` model used is a CNN with 2 convolutional layers and 2 linear layers.\n",
        "\n",
        "Each `wav` file is 30 seconds long and sampled at 22050 Hz. So, we have datapoints of size: ~661500. As humans. We make an estimation from a human standpoint regarding how long 'musical features' are to differentiate genres. The smallest 'features' seem to be differentiable within a significant fraction of a second.\n",
        "\n",
        "So, the receptive field of the convolutional layer of the CNN should cover\n",
        "a significant fraction of a second.\n",
        "\n",
        "What is a significant fraction of a second? This is on hyperparameter tuning to decide. But the conclusion is that very small kernel sizes (such as 3 in `2DConv`) should not apply here since we wouldn't obtain much about the song features itself through essentially 0.0001 seconds of the song.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_eGEAlmMP4Rf",
      "metadata": {
        "id": "_eGEAlmMP4Rf"
      },
      "outputs": [],
      "source": [
        "class WavTrainer(nn.Module):\n",
        "  def __init__(self, l1=256, l2=20):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv_layer_1 = nn.Sequential(nn.Conv1d(1, 32, 20),\n",
        "                                      nn.ReLU(),\n",
        "                                      nn.MaxPool1d(kernel_size=10, stride=10)\n",
        "                                      )\n",
        "\n",
        "    self.conv_layer_2 = nn.Sequential(nn.Conv1d(32, 16, 20),\n",
        "                                      nn.ReLU(),\n",
        "                                      nn.MaxPool1d(kernel_size=10, stride=10)\n",
        "                                      )\n",
        "\n",
        "    self.flatten_layer = nn.Flatten()\n",
        "\n",
        "    self.linear_layer_1 = nn.Sequential(nn.Linear(13248, l1),\n",
        "                                        nn.ReLU())\n",
        "\n",
        "    self.linear_layer_2 = nn.Sequential(nn.Linear(l1, l2),\n",
        "                                        nn.ReLU())\n",
        "\n",
        "    self.classifier = nn.Linear(l2, 10)\n",
        "\n",
        "def forward(self, x):\n",
        "    # First 1D convolution layer\n",
        "    x = self.conv_layer_1(x)\n",
        "    # Second 1D convolution layer\n",
        "    x = self.conv_layer_2(x)\n",
        "\n",
        "    # Linear layer and classifier\n",
        "    x = self.flatten_layer(x)\n",
        "    x = self.linear_layer_1(x)\n",
        "    x = self.linear_layer_2(x)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0XUKm25bZwnD",
      "metadata": {
        "id": "0XUKm25bZwnD"
      },
      "source": [
        "Create routines for training and validation. Perform Hyperparameter Tuning to devise a closer to optimized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ABuRlHHQaZQV",
      "metadata": {
        "id": "ABuRlHHQaZQV"
      },
      "outputs": [],
      "source": [
        "def train_wav_classifier_model(config):\n",
        "\n",
        "    model = WavTrainer(l1=config[\"l1\"], l2=config[\"l2\"])\n",
        "    model.to(DEVICE)\n",
        "    wav_dataset = WAVDataset()\n",
        "\n",
        "    # train model with training dataset, but ray tuner uses validation dataset to tune hyperparameters\n",
        "    train_model(model, DEVICE, config, image_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing\n",
        "\n",
        " Create routine for testing model. The split being used is 80% for training, 10% for validation, and 10% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_wav_classifier_model(best_result):\n",
        "    best_model = WavTrainer(l1=best_result.config[\"l1\"], l2=best_result.config[\"l2\"])\n",
        "    best_model.to(DEVICE)\n",
        "\n",
        "    wav_dataset = WAVDataset()\n",
        "\n",
        "    test_model(best_model, best_result, image_dataset, DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main function\n",
        "\n",
        "Here, we specify the range for the hyperparameters we want Ray Tune to tune on. Run the training of the model using various hyperparameters.\n",
        "\n",
        "Test the model using the best trained model as obtained using Ray Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_wav_classifier():\n",
        "    config = {\n",
        "        \"l1\": tune.choice([k for k in range(100, 1000, 50)]),\n",
        "        \"l2\": tune.choice([j for j in range(5, 30, 5)]),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([l for l in range(5, 50, 5)]),\n",
        "        \"num_epochs\": tune.choice([25, 35, 45])\n",
        "    }\n",
        "\n",
        "    # Only stop trials at least after 20 training iterations\n",
        "    asha_scheduler = ASHAScheduler(time_attr='training_iteration',\n",
        "                                   grace_period=20)\n",
        "\n",
        "    # Adjust resources depending on availability\n",
        "    tuner = tune.Tuner(tune.with_resources(tune.with_parameters(train_wav_classifier_model),\n",
        "                       resources={\"cpu\": 2, \"gpu\": 1}),\n",
        "                       tune_config=tune.TuneConfig(\n",
        "                           metric='loss',\n",
        "                           mode=\"min\",\n",
        "                           scheduler=asha_scheduler,\n",
        "                           num_samples=10,\n",
        "                       ),\n",
        "                       run_config=air.RunConfig(verbose=1),\n",
        "                       param_space=config,)\n",
        "\n",
        "    results = tuner.fit()\n",
        "    best_result = results.get_best_result(\"loss\", \"min\")\n",
        "\n",
        "    test_wav_classifier_model(best_result)\n",
        "\n",
        "run_wav_classifier()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
