{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "44ea56c940cf73cb",
      "metadata": {
        "collapsed": false,
        "id": "44ea56c940cf73cb"
      },
      "source": [
        "# WavClassifier\n",
        "\n",
        "Use the WAV file directly; extract features with a CNN using 1DConv.\n",
        "State-of-the-art audio classifiers use Mel-Spectograms as described in `./mel_spec_classifier.ipynb`, but do not  preserve phase information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "983a6f76208e19a9",
      "metadata": {
        "id": "983a6f76208e19a9"
      },
      "outputs": [],
      "source": [
        "!pip install \"ray[tune]\"\n",
        "import torch\n",
        "from utils import *\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as Data\n",
        "from scipy.io import wavfile\n",
        "from ray import air\n",
        "import os\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YSzR7vjgRYsN",
      "metadata": {
        "id": "YSzR7vjgRYsN"
      },
      "source": [
        "## Mount drive\n",
        "Mount google drive if running on google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qmTopAHiRa0a",
      "metadata": {
        "id": "qmTopAHiRa0a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dec4c015eef3f17d",
      "metadata": {
        "collapsed": false,
        "id": "dec4c015eef3f17d"
      },
      "source": [
        "## Constant parameters used in training\n",
        "\n",
        "Run `setup.sh` to mount Google Drive containing GTZAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "936601a226e691e2",
      "metadata": {
        "id": "936601a226e691e2"
      },
      "outputs": [],
      "source": [
        "GTZAN_WAV = \"/content/drive/MyDrive/GTZAN/Data/genres_original/\"\n",
        "\n",
        "GENRES = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3,\n",
        "          'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8,\n",
        "          'rock': 9}\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8f41e4d044bd12",
      "metadata": {
        "collapsed": false,
        "id": "5b8f41e4d044bd12"
      },
      "source": [
        "## Training\n",
        "\n",
        "Create a `Dataset` for the audio files\n",
        "\n",
        "We split the WAV file into equal halves, for 2 reasons.\n",
        "\n",
        "1. Deal with Curse of Dimensionality. The expectation is that the two halves are not identical, so this would be a cheap method to increase data-points and reduce overfitting. Naturally we have reduced the dimensionality of our input as well. 15 seconds should be more than enough to figure out the genre!\n",
        "2. Ray-Tuner was not able to store `WAVDataset` instances by default due to exceeding size constraints. We can override this by setting a larger size constraint, but this hits two birds with one stone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7815db0d76e8aea2",
      "metadata": {
        "id": "7815db0d76e8aea2"
      },
      "outputs": [],
      "source": [
        "class WAVDataset(Data.Dataset):\n",
        "    def __init__(self):\n",
        "        self.wav = []\n",
        "        self.labels = []\n",
        "\n",
        "        # The WAV files were not all the same size. Obtain the minimum size\n",
        "        # and prune the datapoints accordingly\n",
        "        max_size = 330000\n",
        "\n",
        "        # Go through all songs and tag X (tensor of image), Y as genre.\n",
        "        for genre in os.listdir(GTZAN_WAV):\n",
        "            for song in os.listdir(os.path.join(GTZAN_WAV, genre)):\n",
        "                abs_path = os.path.join(GTZAN_WAV, genre, song)\n",
        "\n",
        "                # Seems like there is a format issue with jazz.00054.wav. Skipping..\n",
        "                if 'jazz.00054.wav' not in abs_path:\n",
        "                  _, data = wavfile.read(abs_path)\n",
        "\n",
        "                  # Split into two WAV files, each covering 15 seconds of music\n",
        "                  data_1, data_2 = np.array_split(data, 2)\n",
        "                  data_1 = data_1[:max_size].astype(np.float32)\n",
        "                  data_2 = data_2[:max_size].astype(np.float32)\n",
        "\n",
        "                  # Convert wav file to tensor\n",
        "                  self.wav.append(torch.from_numpy(np.reshape(data_1, (1, max_size))))\n",
        "                  self.wav.append(torch.from_numpy(np.reshape(data_2, (1, max_size))))\n",
        "\n",
        "                  # Convert genre tag to associated digit\n",
        "                  self.labels.append(GENRES[genre])\n",
        "                  self.labels.append(GENRES[genre])\n",
        "    def __len__(self):\n",
        "        return len(self.wav)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.wav[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda583a987e91248",
      "metadata": {
        "collapsed": false,
        "id": "fda583a987e91248"
      },
      "source": [
        "The `WavTrainer` model used is a CNN with 2 convolutional layers and 2 linear layers.\n",
        "\n",
        "Each `wav` file is 30 seconds long and sampled at 22050 Hz. So, we have datapoints of size: ~661500. As humans. We make an estimation from a human standpoint regarding how long 'musical features' are to differentiate genres. The smallest 'features' seem to be differentiable within a significant fraction of a second.\n",
        "\n",
        "So, the receptive field of the convolutional layer of the CNN should cover\n",
        "a significant fraction of a second.\n",
        "\n",
        "What is a significant fraction of a second? This is on hyperparameter tuning to decide. But the conclusion is that very small kernel sizes (such as 3 in `2DConv`) should not apply here since we wouldn't obtain much about the song features itself through essentially 0.0001 seconds of the song.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_eGEAlmMP4Rf",
      "metadata": {
        "id": "_eGEAlmMP4Rf"
      },
      "outputs": [],
      "source": [
        "class WavTrainer(nn.Module):\n",
        "  def __init__(self, l1=1000, l2=20):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv_layer_1 = nn.Sequential(nn.Conv1d(1, 8, 3),\n",
        "                                      nn.ReLU(),\n",
        "                                      nn.MaxPool1d(kernel_size=10, stride=10)\n",
        "                                      )\n",
        "\n",
        "    self.conv_layer_2 = nn.Sequential(nn.Conv1d(8, 16, 3),\n",
        "                                      nn.ReLU(),\n",
        "                                      nn.MaxPool1d(kernel_size=10, stride=10)\n",
        "                                      )\n",
        "\n",
        "    self.flatten_layer = nn.Flatten()\n",
        "\n",
        "    self.linear_layer_1 = nn.Sequential(nn.Linear(52784, l1),\n",
        "                                        nn.ReLU())\n",
        "\n",
        "    self.linear_layer_2 = nn.Sequential(nn.Linear(l1, l2),\n",
        "                                        nn.ReLU())\n",
        "\n",
        "    self.classifier = nn.Linear(l2, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "      # First 1D convolution layer\n",
        "      x = self.conv_layer_1(x)\n",
        "      # Second 1D convolution layer\n",
        "      x = self.conv_layer_2(x)\n",
        "\n",
        "      # Linear layer and classifier\n",
        "      x = self.flatten_layer(x)\n",
        "      x = self.linear_layer_1(x)\n",
        "      x = self.linear_layer_2(x)\n",
        "      x = self.classifier(x)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0XUKm25bZwnD",
      "metadata": {
        "id": "0XUKm25bZwnD"
      },
      "source": [
        "Create routines for training and validation. Perform Hyperparameter Tuning to devise a closer to optimized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ABuRlHHQaZQV",
      "metadata": {
        "id": "ABuRlHHQaZQV"
      },
      "outputs": [],
      "source": [
        "def train_wav_classifier_model(config):\n",
        "\n",
        "    model = WavTrainer(l1=config[\"l1\"], l2=config[\"l2\"])\n",
        "    model.to(DEVICE)\n",
        "    wav_dataset = WAVDataset()\n",
        "\n",
        "    # train model with training dataset, but ray tuner uses validation dataset to tune hyperparameters\n",
        "    train_model(model, DEVICE, config, wav_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn2DvZljB7qz"
      },
      "source": [
        "## Testing\n",
        "\n",
        " Create routine for testing model. The split being used is 80% for training, 10% for validation, and 10% for testing."
      ],
      "id": "Vn2DvZljB7qz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTUnRL1GB7qz"
      },
      "outputs": [],
      "source": [
        "def test_wav_classifier_model(best_result):\n",
        "    best_model = WavTrainer(l1=best_result.config[\"l1\"], l2=best_result.config[\"l2\"])\n",
        "    best_model.to(DEVICE)\n",
        "\n",
        "    wav_dataset = WAVDataset()\n",
        "\n",
        "    test_model(best_model, best_result, wav_dataset, DEVICE)"
      ],
      "id": "yTUnRL1GB7qz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgyvjLrUB7q0"
      },
      "source": [
        "# Main function\n",
        "\n",
        "Here, we specify the range for the hyperparameters we want Ray Tune to tune on. Run the training of the model using various hyperparameters.\n",
        "\n",
        "Test the model using the best trained model as obtained using Ray Tune"
      ],
      "id": "CgyvjLrUB7q0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf358ntnB7q0"
      },
      "outputs": [],
      "source": [
        "def run_wav_classifier():\n",
        "    config = {\n",
        "        \"l1\": 500,\n",
        "        \"l2\": 20,\n",
        "        \"lr\": 0.001,\n",
        "        \"batch_size\": 35,\n",
        "        \"num_epochs\": 35\n",
        "    }\n",
        "\n",
        "    # Only stop trials at least after 20 training iterations\n",
        "    asha_scheduler = ASHAScheduler(time_attr='training_iteration',\n",
        "                                   grace_period=20)\n",
        "\n",
        "    # Adjust resources depending on availability\n",
        "    tuner = tune.Tuner(tune.with_resources(tune.with_parameters(train_wav_classifier_model),\n",
        "                       resources={\"cpu\": 12, \"gpu\": 1}),\n",
        "                       tune_config=tune.TuneConfig(\n",
        "                           metric='loss',\n",
        "                           mode=\"min\",\n",
        "                           scheduler=asha_scheduler,\n",
        "                           num_samples=1,\n",
        "                       ),\n",
        "                       param_space=config,)\n",
        "\n",
        "    results = tuner.fit()\n",
        "    best_result = results.get_best_result(\"loss\", \"min\")\n",
        "\n",
        "    test_wav_classifier_model(best_result)\n",
        "\n",
        "run_wav_classifier()"
      ],
      "id": "Vf358ntnB7q0"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}